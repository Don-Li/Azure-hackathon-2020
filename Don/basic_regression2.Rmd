---
title: "basic_regression2"
author: "Don Li"
date: "03/06/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Libraries
```{r}
library( data.table )
load( "regression1.RData" )
do_rmse = function( yhat, test_set ){
    rmse = sqrt( mean( (yhat - test_set$timediff)^2 ) )
    rmse
}
```


# Join data
E has computed distances for each trip. These distances were computed by summing over the distance between points for each journey. The Haversine distance was used. Documentation/code is in the `real-distances.py`. The units are in km.

We will join this to the existing summary dataset.

```{r}
new_dist = data.table( read.csv("../distances_km.csv" ) )
new_dist[ , trj_id := as.character(ID) ]
summary_data[ new_dist, new_dist := i.km, on = "trj_id" ]
```


# Updating the model
Previously, I used speed information. But, in our model inputs, we won't have speed, since we are predicting with the origin, the destination, and the time. I will remove the speed variables for the regression model.


```{r}
# Standard remove trip ID and month
summary_data[ , trj_id := NULL ]
summary_data[ , month_ := NULL ]
# Speed removed
summary_data[ , speed_avg := NULL ]
summary_data[ , speed_var := NULL ]
```

## Add some other variables

```{r}
summary_data[ , is_weekend := weekday_ %in% c("Sat", "Sun") ]
summary_data[ , is_rushhour_morning := hour_ > 7 & hour_ < 10 ]
summary_data[ , is_rushhour_night := hour_ > 17 & hour_ < 20 ]
```

```{r}
set.seed(1)
n = nrow( summary_data )
training_set_id = sample( 1:n, n * 0.75 )
training_set = summary_data[ training_set_id ]
test_set = summary_data[ -training_set_id ]
```

# Fit the models

Without speed, our basic model isn't too good. Adjusted R^2 is about 25%. With a proper distance, the regression model is a lot better. Adjusted R^2 is 55%. We can also add both distance measures and see if that improves anything. Summary table below:

```{r warning=FALSE}
model1_vars = setdiff( names(training_set), "new_dist" )
model1 = lm( timediff ~ .*., training_set[,mget(model1_vars)] )
model1_summary = summary(model1)
model1_rmse = do_rmse( predict( model1, test_set ), test_set )

model2_vars = setdiff( names(training_set), "dist_" )
model2 = lm( timediff ~ .*., training_set[,mget(model2_vars)] )
model2_summary = summary(model2)
model2_rmse = do_rmse( predict( model2, test_set ), test_set )

model3 = lm( timediff ~ .*., training_set )
model3_summary = summary(model3)
model3_rmse = do_rmse( predict( model3, test_set ), test_set )
```

```{r}
rmse_list = round( c( model1_rmse, model2_rmse, model3_rmse ), 3 )
adj_r2_list = round( c( model1_summary$adj.r.squared, model2_summary$adj.r.squared,
    model3_summary$adj.r.squared ), 3 )
model_names = c("Total dist", "New dist", "Both dist")
data.table( model_names, rmse_list, adj_r2_list )
```

# Further questions:

## What if we also add the L1 distance?
```{r}
load( "regression1.RData" )
summary_data[ new_dist, new_dist := i.km, on = "trj_id" ]
summary_data[ , is_weekend := weekday_ %in% c("Sat", "Sun") ]
summary_data[ , is_rushhour_morning := hour_ > 7 & hour_ < 10 ]
summary_data[ , is_rushhour_night := hour_ > 17 & hour_ < 20 ]

L1_dist = dataset[ , {
    indices = c(1,.N)
    lat_diff = abs( diff( rawlat[indices] ) )
    lng_diff = abs( diff( rawlng[indices] ) )
    list( L1 = lat_diff + lng_diff )
}, by = "trj_id" ]

summary_data[ L1_dist, L1_dist := i.L1, on = "trj_id" ]
summary_data[ , eval( c("trj_id", "month_", "speed_avg", "speed_var")) := NULL ]

set.seed(1)
n = nrow( summary_data )
training_set_id = sample( 1:n, n * 0.75 )
training_set = summary_data[ training_set_id ]
test_set = summary_data[ -training_set_id ]
```

```{r}
model4 = lm( timediff ~ .*., training_set )
model4_summary = summary(model4)
model4_rmse = do_rmse( predict( model4, test_set ), test_set )
model4_rmse

summary_data[ , L1_dist := NULL ]
training_set[ , L1_dist := NULL ]
```

If we add the L1 distance with everything, the model gets a bit worse.

## Polynomial terms for the distance/time variables

I manually tuned the interactions. In general, it's not good to have x * X^2 * x^3 interaction. Cubic interactions are also not great either.

```{r}
# Remove is_weekend because weekday_ is already a factor. Model is over-specified.
training_set[ , is_weekend := NULL ]
model5 = lm( timediff ~ .*. +
        weekday_ * I(dist_^2) * I(hour_^2) * I(new_dist^2) +
        I(new_dist^3) + I(hour_^3) + I(dist_^3),
    training_set )
model5_summary = summary(model5)
model5_rmse = do_rmse( predict( model5, test_set ), test_set )
model5_rmse
```

Decent improvement over the non-polynomial model.

## Trip distance percentiles

Use a model-based percentile. Log-normal was good from my testing.
```{r}
dist_train = training_set$new_dist

lognorm_loglik = function( theta ){
    if ( any( theta < 0 ) ) return( 1e5 )
    -sum( dlnorm( training_set$new_dist, theta[1], theta[2], log = T ) )
}
lognorm_params = optim( c(1,1), lognorm_loglik )

training_set[ , plot(density(new_dist)) ]
xrange = seq(0,100,by=0.1)
lines(
    xrange,
    dlnorm( xrange, lognorm_params$par[1], lognorm_params$par[2] ),
    col = "red"
)
```

```{r}
training_set[ , dist_quant := {
    plnorm( new_dist, lognorm_params$par[1], lognorm_params$par[2] )
} ]
# Very important to use training quantiles to categorise the test set
test_set[ , dist_quant := {
    plnorm( new_dist, lognorm_params$par[1], lognorm_params$par[2] )
} ]

model6 = lm( timediff ~ .*. +
        weekday_ * I(dist_^2) * I(hour_^2) * I(new_dist^2)+
        I(new_dist^3) + I(hour_^3) + I(dist_^3)+
        dist_quant,
    training_set )
model6_summary = summary(model6)
model6_rmse = do_rmse( predict( model6, test_set ), test_set )
model6_rmse
```

Not much of a change, but we could find a use for this variable somewhere.

# Conclusions

The model so far is a linear model with some polynomial things. I will put the summary of the model at the very end if anyone wants to know what the values of the coefficients were.
```{r eval = FALSE}
model5 = lm( timediff ~ .*. +
        weekday_ * I(dist_^2) * I(hour_^2) * I(new_dist^2) +
        I(new_dist^3) + I(hour_^3) + I(dist_^3),
    training_set )
```

Our RMSE on the test set was `r model5_rmse`. So, about 3mins and 12 seconds average error.

```{r fig.height=4, fig.width=5, warning=FALSE}
yhat = predict( model5, test_set )
par( cex = 1.5 )
hist( abs(yhat - test_set$timediff), main = "Abs errors", 
    xlab = "Absolute prediction errors (s)",
    breaks = 100 )
```

```{r fig.height = 5, fig.width = 5}
par( cex = 1.5 )
total_range = range( c( 0, yhat, test_set$timediff ) )
plot( yhat, test_set$timediff, pch = 16, cex = 0.5,
    main = "Observed vs predicted",
    xlab = "Predicted ETA (s)",
    ylab = "Observed arrival (s)",
    xlim = total_range, ylim = total_range
    )
abline( 0, 1 )
```


```{r}
model5_summary
anova(model5)
```



